{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "3. KnowYourNetwork.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPyB14yWouakoPaW+x8C9vD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rithikJha/AIMechanics/blob/master/3_KnowYourNetwork.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jkcirMJNqW5l",
        "colab_type": "text"
      },
      "source": [
        "# 3. Know Your Network\n",
        "\n",
        "Since you are in 3rd notebook of this series, you must be comfortable with **tensors** and data presented in **Pytorch tensor** . In this notebook we will learn about the **DEEP NEURAL NETWORK** in pytorch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ut9rDtYRqmT9",
        "colab_type": "text"
      },
      "source": [
        "## What we will be doing?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qR5fGLCth5Mc",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "\n",
        "\n",
        "Any Deep Learning pipeline has 4 components - \n",
        "\n",
        "\n",
        "1.   Prepare the data\n",
        "2.   **Build the Model** ( Our concern in this notebook )\n",
        "3.   Train on your model\n",
        "4.   Analyse the model's result\n",
        "\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6LGr-v13rnW4",
        "colab_type": "text"
      },
      "source": [
        "## What does we mean when we say network(or neural network)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_vXlBL9wiAWU",
        "colab_type": "text"
      },
      "source": [
        "By network we just mean, **a function** that maps inputs to outputs. \n",
        "\n",
        "Example -\n",
        "\n",
        "1. Classification\n",
        "\n",
        "   input image ----**f(x)**---> Image contains what?? (Cats or dogs or elephants)\n",
        "\n",
        "2. Regression\n",
        "   \n",
        "   input data -------**f(x)**----> What is the price of the house??\n",
        "\n",
        "   \n",
        "Here **f(x)** is our network that is suppose to approximate/turn into a function, that does the job required (predicting house prices, finding a tumour to be malignant or benign, what object does the image contains? and other such jobs)\n",
        "\n",
        "\n",
        "![alt text](https://www.pyimagesearch.com/wp-content/uploads/2016/08/simple_neural_network_header-768x377.jpg)\n",
        "\n",
        "But in real life, its difficult to build a function that takes a image as a tensor and tells if it is a cat or dog or horse because of soooo many variables that these function depends upon , it is nearly impossible to approximate these function that yeilds great accuracy.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WHFap4cAzlQ9",
        "colab_type": "text"
      },
      "source": [
        "## How deep learning helps us in this problem?\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PZtfwpN3iKka",
        "colab_type": "text"
      },
      "source": [
        "Here enters **deep learning**, a highly iterative process, where your job is only to think about how this function must look like without worrying about the accurate values that would yeild required result.\n",
        "\n",
        "\n",
        "Researchers have been working very hard to just find a suitable look for this function. Mostly by intelligent **hit n trial** methods. But there is always a possiblity that a better **network architecture**(look of the function) can emerge due to enormous time spent in the research. \n",
        "\n",
        "Here are some examples of the **architecture**/looks built to optimize the prediction accuracy.\n",
        "\n",
        "![alt text](http://kim.hfg-karlsruhe.de/wp-content/uploads/2017/12/neural-network-chart-768x506.png)\n",
        "\n",
        "Soon after you comfortable with basics of designing a network and approximate greater results, you must dive in to read research papers (Google it) to bring tweakings to your network so as to improve the accuracy of prediction further.\n",
        "\n",
        "Those tweaks which you will be implementing for the purpose of further improving the accuracy yeild by your network, are discovered by researcher in a highly iterative manner( hit and trial (not completely actually, as they get the approximate idea of what would be better because of their experience) ).\n",
        "\n",
        "Example - The accuracy yeild by normal **Deep Neural Network**(DNN) is not that great, so someone thought why to treat image as whole (as if all locations in the image are equally important), then he went to preserve the spatial information and **Convolutional Neural Networks**(CNN) was born.\n",
        "\n",
        "\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JPWBBvVI1mvs",
        "colab_type": "text"
      },
      "source": [
        "## Components of each network\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IxOD00XiiSaM",
        "colab_type": "text"
      },
      "source": [
        "In this notebook we will be decoupling the network and look at each component closely.\n",
        "\n",
        "A network architecture is just a collection of **Layers**. What each layer does is basically transforms the incoming tensor in some way using **Weights**, **biases**, **activations** and other transformation operations like **pooling**, **regularizations**, **Normalizations** etc. and yeilds us an output tensor which is then passed to next layer. \n",
        "\n",
        "This coming of tensor and undergoing some transformation to become some other tensor, is called a **forward pass** of that tensor through that layer. Each forward pass yeild us some transformation( except if its a dummy layer which yeilds no transformations ).\n",
        "\n",
        "We can think of our network as a **big layer**, where input data tensor undergoes a **big transformation**(collection of all transformation of each sub layer). The forward pass( undergoing big transformation ) through our complete network is **Forward Propagation**.\n",
        "\n",
        "Our input tensor flows through these layers, undergoing various transformation and yeilding us an output tensor.\n",
        "\n",
        "![alt text](http://www.wildml.com/wp-content/uploads/2015/11/Screen-Shot-2015-11-07-at-7.26.20-AM-1024x279.png)\n",
        "\n",
        "\n",
        "Here we can see that the boat image is undergoing through transformations and yeilding us the probability of it being a dog, cat, boat or bird.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "The process of conversion of a network from a **look-alike** function to an **accurate** function is called **learning** . You will hear people saying that their network is learning.\n",
        "\n",
        "**Look-Alike**----------------------------------------->**Accurate** (Well !! something must be changing in the network to account for this change)\n",
        "\n",
        "The only things that changes in a network while *the network is learning* are **Weights and Biases**. \n",
        "\n",
        "We have seen that Weights and biases are responsible for transformation of a tensor during forward pass. So, if these weights and biases changes, the forward pass will yeild us different transformation than before. The purpose of *learning* is to find appropriate values for these *weights and biases* , so that after learning, the forward pass will yeild us more appropriate transformations.\n",
        "\n",
        "Let's see what weights and biases are - \n",
        "\n",
        "**Weights and biases** are the attributes of each layer and are contained within each layer in pytorch. They are also called **learnable parameters** because those are the only things that undergoes changes when network is learning. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C9sH2lxBiZaD",
        "colab_type": "text"
      },
      "source": [
        "## How to make neural networks in pytorch?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0e0Tr0RGgoaV",
        "colab_type": "text"
      },
      "source": [
        "We are talking about making our own custom network which will be a combination of *already implemented layers* present in torch.nn Class of pytorch. \n",
        "\n",
        "**We use deep learning library just for this purpose, so that all the standard transformations ( forward passes through standard types of layers ) comes predefines to us.**\n",
        "\n",
        "Let's see how pytorch implements their layers. And we will use this to implement our **big layer**/neural network later.\n",
        "\n",
        " Eg- Linear Layer (Edited pytorch source code)\n",
        "\n",
        "```\n",
        "class Linear(Module):\n",
        "    r\"\"\"Applies a linear transformation to the incoming data: :math:`y = xA^T + b`\n",
        "\n",
        "    Examples::\n",
        "\n",
        "        >>> m = nn.Linear(20, 30)\n",
        "        >>> input = torch.randn(128, 20)\n",
        "        >>> output = m(input)\n",
        "        >>> print(output.size())\n",
        "        torch.Size([128, 30])\n",
        "    \"\"\"\n",
        "    __constants__ = ['in_features', 'out_features']\n",
        "    in_features: int\n",
        "    out_features: int\n",
        "    weight: Tensor\n",
        "\n",
        "    def __init__(self, in_features: int, out_features: int, bias: bool = True) -> None:\n",
        "        super(Linear, self).__init__()\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        self.weight = Parameter(torch.Tensor(out_features, in_features))\n",
        "        if bias:\n",
        "            self.bias = Parameter(torch.Tensor(out_features))\n",
        "        else:\n",
        "            self.register_parameter('bias', None)\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def forward(self, input: Tensor) -> Tensor:\n",
        "        return F.linear(input, self.weight, self.bias)\n",
        "\n",
        "```\n",
        "Let's understand the pytorch mechanics.\n",
        "\n",
        "Every Layer essentially does these three things-  \n",
        "\n",
        "1. \n",
        "\n",
        "```\n",
        "class Linear(Module)\n",
        "``` \n",
        "Extending from torch.nn.Module so as to utilise the power of forward(), parameters() etc.\n",
        "\n",
        "\n",
        "\n",
        "2. \n",
        "\n",
        "```\n",
        "self.weight = Parameter(torch.Tensor(out_features, in_features))\n",
        "        if bias:\n",
        "            self.bias = Parameter(torch.Tensor(out_features))\n",
        "```\n",
        "Here layer is wrapping its attributes in torch.nn.parameter class or in other words, it is **registering** it to be the learnable parameters which is an attribute of torch.nn.Module. Parameters are Tensor subclasses, that have a very special property when used with Module - when they’re assigned as Module attributes(here it means layer's attributes) they are automatically added to the list of its parameters (module's parameters implementations are extended by our custom network), and will appear e.g. in parameters() iterator. Assigning a Tensor doesn’t have such effect. Also **Dimension of weight is [out_features, in_features]**\n",
        "\n",
        "3. \n",
        "```\n",
        "    def forward(self, input: Tensor) -> Tensor:\n",
        "        return F.linear(input, self.weight, self.bias)\n",
        "```\n",
        "We can see that weights and biases are attribute of Linear class (torch.nn.Linear) which inherits from torch.nn.Module , and this it has to override the **forward function**. It is doing that and returning a function from torch.nn.functional class.\n",
        "\n",
        "Let's see what it means to undergo forward pass through linear layer means.\n",
        "\n",
        "```\n",
        "def linear(input, weight, bias=None):\n",
        "    # type: (Tensor, Tensor, Optional[Tensor]) -> Tensor\n",
        "    \"\"\"\n",
        "        Applies a linear transformation to the incoming data: :math:`y = xA^T + b`.\n",
        "    \"\"\"\n",
        "    tens_ops = (input, weight)\n",
        "    if not torch.jit.is_scripting():\n",
        "        if any([type(t) is not Tensor for t in tens_ops]) and has_torch_function(tens_ops):\n",
        "            return handle_torch_function(linear, tens_ops, input, weight, bias=bias)\n",
        "    if input.dim() == 2 and bias is not None:\n",
        "        # fused op is marginally faster\n",
        "        ret = torch.addmm(bias, input, weight.t())\n",
        "    else:\n",
        "        output = input.matmul(weight.t())\n",
        "        if bias is not None:\n",
        "            output += bias\n",
        "        ret = output\n",
        "    return ret\n",
        "```\n",
        "\n",
        "the above code can be understood through following diagram.\n",
        "\n",
        "![alt text](https://rgl.s3.eu-central-1.amazonaws.com/media/uploads/wjakob/2017/12/05/neuralnetwork_single_neuron.png)\n",
        "\n",
        "input, weight, bias - passed as argument(all are tensor). \n",
        "\n",
        "**output = input.matmul(weight.t()) + bias** ----> this output is basically returned which is also a tensor.\n",
        "\n",
        "\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BBwbv5jNaLNB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Clm2hS0nkrma",
        "colab_type": "text"
      },
      "source": [
        "## Let's make our own **big layer** (Neural Network)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uigAc3GfUodm",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "Now we will be making our custom network i.e. **big layer**( i.e. combination of layers present in torch.nn like linear, conv2d, avg_pool etc.).\n",
        "\n",
        "We have seen that a layer must do three things (int this case our network/**big layer**) - \n",
        "1. **Extend from torch.nn.Module** - nn.Module has methods like - **forward()** which defines forward pass, **parameters()** which returns an iterator over learnable parameters.\n",
        "\n",
        "2. **Wrap its learnable parameters to Parameter class** - Since the learnable parameters of **big layer** is just the collection of learnable parameters of small layers which makes this **big layer**/network. Therefore, all of these *learnable parameters* are already wrapped in torch.nn.parameter class. So, we won't have to do so in case we are using these already implemented torch.nn.Layers.\n",
        "\n",
        "3. **Override forward method** - forward pass for each layer is already overrided within each layer but the *forward pass of the big layer* (**forward propagation**) is not defined yet. So, We must define how the tensor must flow through our network (sequentially? or jumping and skipping layers?? etc etc. )\n",
        "\n",
        "\n",
        "Let's go ahead and implement it in pytorch - \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zl3FOMVzYbN5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class network(nn.Module): # 1. Extending from nn.Module class\n",
        "  def __init__(self):     # This is called a constructor, helps in assigning the instance of a class with attributes defined inside it.\n",
        "    super().__init__()    # \n",
        "    self.layer1 = nn.Linear(in_features = 5, out_features = 10)  # 2. Linear layer 1 (Weights and biases are registered automatically by nn.Module)\n",
        "    self.layer2 = nn.Linear(in_features = 10, out_features = 2)  # 2. Linear layer 2 (Weights and biases are registered automatically by nn.Module)\n",
        "                                                                 # As nn.Module registers all the instances of nn.Parameter class\n",
        "                                                                 # Our layer wraps weights and biases into nn.Parameter class\n",
        "\n",
        "  def forward(self, x): # 3. Overriding forward method to show how must the tensor flow through our network\n",
        "    x = self.layer1(x)  # Transformation through layer 1\n",
        "    x = self.layer2(x)  # Transformation through layer 2\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nXFNwRqFf7NR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        },
        "outputId": "3e53af5e-d18b-4463-8003-99842cc20bf9"
      },
      "source": [
        "model = network() # Making a instance of our network, different data may flow through different instances\n",
        "print(model)      # nn.Module has overidden __repr__() which affects the print function"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "network(\n",
            "  (layer1): Linear(in_features=5, out_features=10, bias=True)\n",
            "  (layer2): Linear(in_features=10, out_features=2, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xX0UyyN0gFqr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "bf47b756-c734-4051-ba3a-31e8f8c608a3"
      },
      "source": [
        "input = torch.rand(100,5)                            # Generating a random tensor to witness the tranformation that our network yeilds\n",
        "output = model.forward(input)                        # Passing a tensor(100,5) through our network for forward propagation\n",
        "print(\"The shape of input tensor is \",input.shape)   \n",
        "print(\"The shape of output tensor is \",output.shape) # It is expected that it will be of size [100,2]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The shape of input tensor is  torch.Size([100, 5])\n",
            "The shape of output tensor is  torch.Size([100, 2])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UCWlOjqNgQQe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "3dbd9fff-e9e8-4905-8547-dc23f81503a8"
      },
      "source": [
        "input2 = torch.rand(500,5)\n",
        "output2 = model(input2) # Notice how we are calling the forward function of our network.\n",
        "                        # It happens because nn.Module also overrides __call__() default function of python.\n",
        "                        # It makes instance of classes callable. \n",
        "                        # In this case, when instance is called, forward function is invoked.\n",
        "print(\"The shape of input tensor is \",input2.shape) \n",
        "print(\"The shape of output tensor is \",output2.shape) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The shape of input tensor is  torch.Size([500, 5])\n",
            "The shape of output tensor is  torch.Size([500, 2])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZYmuBqD-nFyo",
        "colab_type": "text"
      },
      "source": [
        "Let's go ahead and inspect weights and biases which is expected to be contained in each layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "15FNYQ5TgS-0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "2e1fb499-ffef-4430-9ec0-125f0ba1ae50"
      },
      "source": [
        "print(\"Weight for layer l is of dimension [out_feature,in_feature]\")\n",
        "print(\"Layer 1 weights:- \",model.layer1.bias.shape)\n",
        "print(\"Layer 2 weights:- \",model.layer2.weight.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Weight for layer l is of dimension [out_feature,in_feature]\n",
            "Layer 1 weights:-  torch.Size([10])\n",
            "Layer 2 weights:-  torch.Size([2, 10])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JLxGyQ5OnVbh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        },
        "outputId": "42c88001-e7d3-4c2e-9b35-2b392bcaf06c"
      },
      "source": [
        "# We have seen that weights are registered as parameters of nn.module, let's see how we can access it without the help of layer\n",
        "x = model.parameters() # A function present in nn.Module which returns an iterator over its parameters\n",
        "iter = next(x)         # Parameters are just special tensors in nn.Module whose tracking is done by the network\n",
        "print(iter)            # Parameter class extends from tensor class (So, Every parameter of network is just a tensor but special one)\n",
        "                       # Parameter class also overrides __repr__(), that why we don't get tensor representation on printing the weight\n",
        "                       # Note the extra line \"Parameter containing:\", this shows how weights are special tensors."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Parameter containing:\n",
            "tensor([[ 0.0804, -0.1029,  0.2460,  0.0537, -0.1454],\n",
            "        [ 0.0417,  0.2886, -0.3899,  0.0274,  0.0948],\n",
            "        [ 0.0091,  0.1687, -0.3315,  0.1854, -0.4071],\n",
            "        [ 0.3136, -0.0224,  0.2963, -0.3472, -0.1303],\n",
            "        [ 0.1397,  0.2461, -0.0257, -0.3231, -0.0745],\n",
            "        [-0.3723, -0.3247, -0.2792,  0.3368,  0.3552],\n",
            "        [-0.2218, -0.1265,  0.1949, -0.4301, -0.0656],\n",
            "        [-0.4380, -0.4365, -0.3661, -0.3785, -0.4132],\n",
            "        [ 0.1279,  0.3702,  0.3009,  0.1315,  0.0290],\n",
            "        [ 0.3986, -0.0950,  0.3000,  0.4077,  0.1804]], requires_grad=True)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9CmzqfhvoNWl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "6f8c2663-12f4-4078-dcf7-0037d6609106"
      },
      "source": [
        "# Let's do an intresting thing, forward propagation in multiple way.\n",
        "input2 = torch.rand(100,5)\n",
        "\n",
        "# Way 1 - Let model tackel the transformation\n",
        "outputModel = model(input2)\n",
        "\n",
        "# Way 2 - Extracting required tensor from the model and doing mathematical operation\n",
        "outputL1 = input2.matmul(model.layer1.weight.t()) + model.layer1.bias.t() # Broadcasting\n",
        "outputL2 = outputL1.matmul(model.layer2.weight.t()) + model.layer2.bias.t()\n",
        "print(outputL2.eq(outputModel).sum() == outputModel.numel()) \n",
        "\n",
        "# Way 3 - Use the iterator which provides us with neccesary parameters(special tensors = weights n biases)\n",
        "# Try to decode what an iterator means \n",
        "for i,params in enumerate(model.parameters()):\n",
        "  if i%2 == 0:\n",
        "    input2 = input2.matmul(params.t())\n",
        "  else: \n",
        "    input2 = input2.add(params.t())\n",
        "print(input2.eq(outputModel).sum() == outputModel.numel()) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(True)\n",
            "tensor(True)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pnHVOf1NtXPJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}